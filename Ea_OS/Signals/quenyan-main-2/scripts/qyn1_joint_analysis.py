"""Offline analyser for joint token/payload event streams.

This utility consumes the artefacts generated by ``qyn1_joint_profile`` and
computes empirical entropies alongside the observed bit costs of the encoded
packages. It is intentionally light on dependencies so that it can be run in
batch environments after the profiling step has completed.
"""

from __future__ import annotations

import argparse
import json
import logging
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Mapping, MutableMapping, Optional

from qyn1.event_logging import EncodingEvent, EventPayloadClass
from qyn1.format import PAYLOAD_MAGIC, decode_frame, decode_sections
from qyn1.measurement import conditional_entropy, entropy
from qyn1.package import read_package
from qyn1.string_table import StringTable


logger = logging.getLogger("qyn1_joint_analysis")


@dataclass
class _EventCounters:
    token_counter: Counter[str]
    joint_counter: Counter[tuple[str, tuple[str, str | None, str | None]]]
    payload_class_counters: MutableMapping[EventPayloadClass, Counter[tuple[str, str]]]


@dataclass
class _SectionBits:
    tokens: int
    string_table: int
    metadata: int
    string_table_entries: int


def _load_events(path: Path) -> List[EncodingEvent]:
    events: List[EncodingEvent] = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            events.append(EncodingEvent.from_dict(json.loads(line)))
    return events


def _update_counters(events: Iterable[EncodingEvent], counters: _EventCounters) -> None:
    for event in events:
        counters.token_counter[event.token_key] += 1
        counters.joint_counter[(event.token_key, event.payload_key())] += 1
        payload_id = json.dumps(event.payload_key(), sort_keys=True)
        counters.payload_class_counters[event.payload_class][
            (event.token_key, payload_id)
        ] += 1


def _extract_section_bits(package_bytes: bytes, passphrase: str) -> _SectionBits:
    envelope = read_package(package_bytes, passphrase)
    payload_frame, remainder = decode_frame(envelope.payload, expected_magic=PAYLOAD_MAGIC)
    if remainder:
        raise ValueError("unexpected trailing data after payload frame")
    tokens = 0
    string_table = 0
    metadata = 0
    table_entries = 0
    for section in decode_sections(payload_frame.body):
        if section.identifier == 0x0003:
            tokens = len(section.payload) * 8
        elif section.identifier == 0x0004:
            string_table = len(section.payload) * 8
            try:
                table_entries = len(StringTable.from_bytes(section.payload))
            except Exception:
                logger.warning("Failed to decode string table; entry count unavailable")
        elif section.identifier == 0x0007:
            metadata = len(section.payload) * 8
    return _SectionBits(tokens=tokens, string_table=string_table, metadata=metadata, string_table_entries=table_entries)


def _load_dataset_events(dataset_root: Path, file_records: Iterable[Mapping[str, object]]) -> List[EncodingEvent]:
    all_events: List[EncodingEvent] = []
    for record in file_records:
        path_value = record.get("path")
        if not isinstance(path_value, str):
            continue
        events_path = dataset_root / "events" / Path(path_value).with_suffix(".events.jsonl")
        if not events_path.exists():
            logger.warning("Event log missing for %s", events_path)
            continue
        all_events.extend(_load_events(events_path))
    return all_events


def _analyse_datasets(results: Mapping[str, object], output_root: Path, passphrase: str) -> Dict[str, object]:
    token_counter: Counter[str] = Counter()
    joint_counter: Counter[tuple[str, tuple[str, str | None, str | None]]] = Counter()
    payload_class_counters: MutableMapping[EventPayloadClass, Counter[tuple[str, str]]] = defaultdict(Counter)

    total_tokens = 0
    bits_by_mode: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    string_entries_by_mode: Dict[str, List[int]] = defaultdict(list)

    datasets = results.get("datasets")
    if not isinstance(datasets, list):
        raise ValueError("results file is missing the 'datasets' array")

    for dataset in datasets:
        if not isinstance(dataset, Mapping):
            continue
        slug = dataset.get("dataset")
        files = dataset.get("files", [])
        if not isinstance(slug, str) or not isinstance(files, list):
            continue
        dataset_root = output_root / slug
        events = _load_dataset_events(dataset_root, files)
        counters = _EventCounters(token_counter, joint_counter, payload_class_counters)
        _update_counters(events, counters)
        for record in files:
            path_value = record.get("path") if isinstance(record, Mapping) else None
            if not isinstance(path_value, str):
                continue
            token_count = int(record.get("token_count", 0))
            total_tokens += token_count
            relative = Path(path_value)
            compressed = record.get("compressed_bytes", {})
            if not isinstance(compressed, Mapping):
                continue
            for mode, _ in compressed.items():
                package_path = dataset_root / mode / relative.with_suffix(".qyn1")
                if not package_path.exists():
                    logger.warning("Package missing for %s", package_path)
                    continue
                bits = _extract_section_bits(package_path.read_bytes(), passphrase)
                bits_by_mode[mode]["tokens"] += bits.tokens
                bits_by_mode[mode]["strings"] += bits.string_table
                bits_by_mode[mode]["metadata"] += bits.metadata
                if bits.string_table_entries:
                    string_entries_by_mode[mode].append(bits.string_table_entries)

    token_entropy = entropy(token_counter)
    joint_entropy = entropy(joint_counter)

    payload_conditional_entropy: Dict[str, float] = {}
    for payload_class, counter in payload_class_counters.items():
        payload_conditional_entropy[payload_class.value] = conditional_entropy(counter)

    mode_summaries: Dict[str, Dict[str, float | int]] = {}
    for mode, entries in bits_by_mode.items():
        mode_summary: Dict[str, float | int] = {
            "tokens_bits": entries["tokens"],
            "string_table_bits": entries["strings"],
            "metadata_bits": entries["metadata"],
        }
        if total_tokens:
            mode_summary["bits_per_token"] = (entries["tokens"] + entries["strings"]) / total_tokens
        average_entries = (
            sum(string_entries_by_mode[mode]) / len(string_entries_by_mode[mode])
            if string_entries_by_mode[mode]
            else 0
        )
        mode_summary["avg_string_table_entries"] = average_entries
        mode_summaries[mode] = mode_summary

    payload_entropy_total = sum(payload_conditional_entropy.values())

    return {
        "token_entropy": token_entropy,
        "joint_entropy": joint_entropy,
        "payload_conditional_entropy": payload_conditional_entropy,
        "total_tokens": total_tokens,
        "modes": mode_summaries,
        "conditional_entropy_total": payload_entropy_total,
        "joint_entropy_via_chain_rule": token_entropy + payload_entropy_total,
    }


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="Analyse joint event logs and package costs")
    parser.add_argument("output_root", type=Path, help="Directory containing profiler output")
    parser.add_argument(
        "--summary",
        type=Path,
        default=Path("joint_profile_results.json"),
        help="Summary JSON produced by qyn1_joint_profile",
    )
    parser.add_argument(
        "--passphrase",
        default="joint-profile",
        help="Passphrase used to decrypt packages",
    )
    parser.add_argument(
        "--results",
        type=Path,
        default=Path("joint_profile_analysis.json"),
        help="Where to write the aggregated analysis",
    )
    args = parser.parse_args(argv)

    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    if not args.summary.exists():
        logger.error("Summary file %s does not exist", args.summary)
        return 1

    summary = json.loads(args.summary.read_text())
    try:
        analysis = _analyse_datasets(summary, args.output_root, args.passphrase)
    except Exception as exc:  # pragma: no cover - defensive logging for CLI
        logger.exception("Failed to analyse datasets: %s", exc)
        return 1
    args.results.write_text(json.dumps(analysis, indent=2))
    logger.info("Wrote analysis to %s", args.results)
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entrypoint
    raise SystemExit(main())
