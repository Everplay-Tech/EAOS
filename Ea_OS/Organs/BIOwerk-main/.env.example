# ==============================================================================
# BIOwerk Environment Configuration Template
# ==============================================================================
#
# ⚠️  SECURITY NOTICE: This file contains NO real secrets ⚠️
#
# Placeholders like <GENERATE_WITH_openssl_rand_-base64_32> must be replaced
# with actual strong, randomly generated passwords before use.
#
# SETUP INSTRUCTIONS:
# 1. Copy this file: cp .env.example .env
# 2. Generate secrets using provided commands
# 3. Replace ALL placeholders with generated values
# 4. Verify: ./scripts/check_secrets.sh
#
# ==============================================================================

# BIOwerk Configuration
# Copy this file to .env and update with your values

# ============================================================================
# PostgreSQL Configuration
# ============================================================================
# When using PgBouncer (recommended for production):
# - Set POSTGRES_HOST=pgbouncer and POSTGRES_PORT=6432
# When connecting directly to PostgreSQL (development only):
# - Set POSTGRES_HOST=postgres and POSTGRES_PORT=5432
POSTGRES_HOST=pgbouncer
POSTGRES_PORT=6432
POSTGRES_USER=biowerk
POSTGRES_PASSWORD=<GENERATE_WITH_openssl_rand_-base64_32>
POSTGRES_DB=biowerk

# ============================================================================
# PgBouncer Connection Pooling Configuration
# ============================================================================
# Enable PgBouncer for production-grade connection pooling
# Reduces PostgreSQL memory usage and improves connection reuse

# Pool Mode (transaction | session | statement)
# - transaction: Recommended for microservices (best connection reuse)
#   Connection returned to pool after each transaction
# - session: Traditional mode, connection held until client disconnects
# - statement: Aggressive pooling, connection returned after each statement
PGBOUNCER_POOL_MODE=transaction

# Connection Limits
# Maximum client connections (from application services)
# Formula: (num_services * connections_per_service) + overhead
PGBOUNCER_MAX_CLIENT_CONN=200

# Default pool size (server connections to PostgreSQL)
# This is the main pool of connections kept alive
# Start with expected concurrent queries / databases
PGBOUNCER_DEFAULT_POOL_SIZE=25

# Minimum pool size (always maintained)
# Ensures warm connections are available
PGBOUNCER_MIN_POOL_SIZE=10

# Reserve pool (emergency connections)
# Extra connections when default pool exhausted
PGBOUNCER_RESERVE_POOL_SIZE=10

# Maximum database connections (including reserve)
# Should be less than PostgreSQL max_connections (default: 100)
PGBOUNCER_MAX_DB_CONNECTIONS=50

# Timeouts (seconds)
# Server idle timeout - close idle connections
PGBOUNCER_SERVER_IDLE_TIMEOUT=600

# Logging
# Set to 1 to log connections/disconnections, 0 to disable
PGBOUNCER_LOG_CONNECTIONS=1
PGBOUNCER_LOG_DISCONNECTIONS=1

# ============================================================================
# MongoDB Configuration
# ============================================================================
MONGO_HOST=mongodb
MONGO_PORT=27017
MONGO_INITDB_ROOT_USERNAME=biowerk
MONGO_INITDB_ROOT_PASSWORD=<GENERATE_WITH_openssl_rand_-base64_32>
MONGO_INITDB_DATABASE=biowerk

# ============================================================================
# Redis Configuration
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# ============================================================================
# Cache Configuration
# ============================================================================
CACHE_TTL=300
CACHE_ENABLED=true

# ============================================================================
# Application Configuration
# ============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=development

# ============================================================================
# Authentication Configuration
# ============================================================================
# IMPORTANT: Change JWT_SECRET_KEY in production! Use: openssl rand -hex 32
JWT_SECRET_KEY=<GENERATE_WITH_openssl_rand_-hex_32>
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7
API_KEY_HEADER=X-API-Key
REQUIRE_AUTH=false

# ============================================================================
# TLS/HTTPS Configuration
# ============================================================================
# Enable TLS/HTTPS for production (requires valid certificates)
# For development, generate self-signed certs: python scripts/generate_certs.py
TLS_ENABLED=false
TLS_CERT_FILE=./certs/cert.pem
TLS_KEY_FILE=./certs/key.pem
TLS_CA_FILE=
TLS_VERIFY_CLIENT=false
# Minimum TLS version: TLSv1.2 or TLSv1.3 (TLSv1.3 recommended for production)
TLS_MIN_VERSION=TLSv1.2
# Custom cipher suite (leave empty for secure defaults)
TLS_CIPHERS=

# ============================================================================
# Rate Limiting Configuration
# ============================================================================
# Enable rate limiting to prevent abuse and DDoS attacks
RATE_LIMIT_ENABLED=true
# Number of requests allowed per time window
RATE_LIMIT_REQUESTS=100
# Time window in seconds
RATE_LIMIT_WINDOW=60
# Strategy: fixed_window, sliding_window (recommended), or token_bucket
RATE_LIMIT_STRATEGY=sliding_window
# Apply rate limiting per IP address
RATE_LIMIT_PER_IP=true
# Apply rate limiting per authenticated user
RATE_LIMIT_PER_USER=true
# Burst size for token bucket strategy (allows temporary bursts)
RATE_LIMIT_BURST=20
# Paths to exclude from rate limiting (comma-separated)
# RATE_LIMIT_EXCLUDE_PATHS=/health,/metrics

# ============================================================================
# Agent URLs (for mesh gateway)
# ============================================================================
AGENT_OSTEON_URL=http://osteon:8001
AGENT_MYOCYTE_URL=http://myocyte:8002
AGENT_SYNAPSE_URL=http://synapse:8003
AGENT_CIRCADIAN_URL=http://circadian:8004
AGENT_NUCLEUS_URL=http://nucleus:8005
AGENT_CHAPERONE_URL=http://chaperone:8006

# ============================================================================
# Audit Logging Configuration
# ============================================================================
# Enable comprehensive audit logging with encryption at rest
AUDIT_ENABLED=true
# Log all API requests (captures method, path, headers, body)
AUDIT_LOG_REQUESTS=true
# Log all API responses (captures status, headers, body)
AUDIT_LOG_RESPONSES=true
# Encrypt sensitive fields (IP addresses, request/response data)
AUDIT_ENCRYPT_SENSITIVE=true
# Default retention period in days (1 year)
AUDIT_RETENTION_DAYS=365
# Retention for authentication events (90 days)
AUDIT_RETENTION_AUTH_DAYS=90
# Retention for data modification events (7 years for compliance)
AUDIT_RETENTION_DATA_DAYS=2555
# Retention for security events (2 years)
AUDIT_RETENTION_SECURITY_DAYS=730
# Collect geolocation data (requires external GeoIP service)
AUDIT_COLLECT_GEO=false
# Maximum size for request/response fields (64KB)
AUDIT_MAX_FIELD_SIZE=65536
# Batch size for bulk audit log writes
AUDIT_BATCH_SIZE=100
# Write audit logs asynchronously for better performance
AUDIT_ASYNC_WRITE=true

# ============================================================================
# Encryption Configuration (for Audit Logs and Sensitive Data)
# ============================================================================
# Enable encryption at rest for sensitive data
ENCRYPTION_ENABLED=true
# IMPORTANT: Master encryption key - MUST be changed in production!
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# In production, use KMS (AWS KMS, Azure Key Vault, HashiCorp Vault)
ENCRYPTION_MASTER_KEY=<GENERATE_WITH_openssl_rand_-base64_32>
# Current encryption key version (increment when rotating keys)
ENCRYPTION_KEY_VERSION=1
# Days before key rotation is recommended (90 days)
ENCRYPTION_KEY_ROTATION_DAYS=90
# Base64-encoded salt for key derivation (auto-generated if not set)
ENCRYPTION_SALT=
# Encryption algorithm (DO NOT CHANGE unless you know what you're doing)
ENCRYPTION_ALGORITHM=AES-256-GCM

# ============================================================================
# Data Retention Policy Configuration (SOC2, HIPAA, GDPR Compliance)
# ============================================================================
# Enable automated data retention policy enforcement
RETENTION_ENABLED=true

# Automated policy evaluation interval (hours)
# How often to check and enforce retention policies
RETENTION_EVALUATION_INTERVAL_HOURS=24

# Archive cleanup interval (hours)
# How often to delete expired archives
RETENTION_ARCHIVE_CLEANUP_INTERVAL_HOURS=168

# Default retention periods by data type (days)
# These are overridden by specific retention policies
RETENTION_DEFAULT_AUDIT_LOG_DAYS=365
RETENTION_DEFAULT_USER_DATA_DAYS=2555
RETENTION_DEFAULT_EXECUTION_DAYS=90
RETENTION_DEFAULT_PROJECT_DAYS=730
RETENTION_DEFAULT_ARTIFACT_DAYS=730
RETENTION_DEFAULT_API_KEY_DAYS=90
RETENTION_DEFAULT_SESSION_DAYS=30
RETENTION_DEFAULT_CACHE_DAYS=7

# Archive expiration period (days)
# How long to keep archived data before permanent deletion
# Set to 0 to keep archives indefinitely
RETENTION_ARCHIVE_EXPIRATION_DAYS=2555

# Require archival before deletion
# If true, all deletions must be preceded by archival
RETENTION_REQUIRE_ARCHIVE_BEFORE_DELETE=true

# Enable legal hold support
# Allows administrators to prevent data deletion for litigation/investigation
RETENTION_LEGAL_HOLD_ENABLED=true

# Compliance framework notifications
# Send alerts for compliance violations and policy conflicts
RETENTION_COMPLIANCE_ALERTS_ENABLED=true

# Dry run mode (for testing)
# If true, policies are evaluated but no data is actually deleted
RETENTION_DRY_RUN=false

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Primary LLM provider: 'openai', 'anthropic', 'deepseek', 'ollama', or 'local'
# - 'local': Use standalone GGUF models in service directories (NO git commit)
# - 'ollama': Use shared Ollama server (current default)
# - 'openai', 'anthropic', 'deepseek': Cloud APIs (requires keys)
LLM_PROVIDER=ollama

# OpenAI Configuration
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OPENAI_TIMEOUT=60

# Anthropic Claude Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_MAX_TOKENS=4096
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_TIMEOUT=60

# DeepSeek Configuration
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MAX_TOKENS=4096
DEEPSEEK_TEMPERATURE=0.7
DEEPSEEK_TIMEOUT=60

# Ollama Configuration (Local/Open-Source LLMs)
# Recommended models: phi3:mini, llama3.2, mistral, qwen2.5:7b
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=phi3:mini
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7
OLLAMA_TIMEOUT=120

# Local Model Configuration (Standalone GGUF files - NOT in git)
# Download models using: ./scripts/download-models.sh
# Each service has its own copy in services/SERVICE_NAME/models/
LOCAL_MODEL_PATH=./models
LOCAL_MODEL_NAME=phi3-mini
LOCAL_MODEL_FILE=model.gguf
LOCAL_MAX_TOKENS=4096
LOCAL_TEMPERATURE=0.7
LOCAL_CONTEXT_SIZE=4096
LOCAL_GPU_LAYERS=0

# ============================================================================
# OpenTelemetry / Distributed Tracing Configuration
# ============================================================================
# Enable OpenTelemetry distributed tracing across all services
OTEL_ENABLED=true

# Service name (will be prefixed with service-specific names automatically)
OTEL_SERVICE_NAME=biowerk

# Exporter type: 'otlp', 'jaeger', 'console', or 'none'
# - 'otlp': Standard OpenTelemetry Protocol (production recommended)
# - 'jaeger': Jaeger-native protocol
# - 'console': Print traces to console (development/debugging)
# - 'none': Disable trace export (tracing still happens locally)
OTEL_EXPORTER_TYPE=otlp

# OTLP/Jaeger collector endpoint
# For OTLP gRPC: http://jaeger:4317
# For OTLP HTTP: http://jaeger:4318
# For Jaeger Thrift: http://jaeger:14268
OTEL_EXPORTER_ENDPOINT=http://jaeger:4317

# OTLP protocol: 'grpc' or 'http/protobuf'
OTEL_EXPORTER_PROTOCOL=grpc

# Trace sampling ratio (0.0 to 1.0)
# 1.0 = 100% of traces (development)
# 0.1 = 10% of traces (high-traffic production)
# 0.01 = 1% of traces (very high traffic)
OTEL_SAMPLING_RATIO=1.0

# Enable trace context in logs (correlate logs with traces)
OTEL_LOG_CORRELATION=true

# Span export configuration
OTEL_EXPORT_TIMEOUT=30
OTEL_MAX_QUEUE_SIZE=2048
OTEL_MAX_EXPORT_BATCH_SIZE=512

# Automatic instrumentation toggles
# Instrument database calls (PostgreSQL, MongoDB)
OTEL_INSTRUMENT_DB=true
# Instrument HTTP client calls (httpx)
OTEL_INSTRUMENT_HTTP=true
# Instrument Redis calls
OTEL_INSTRUMENT_REDIS=true

# ============================================================================
# Health Check Configuration (Kubernetes/Docker Probes)
# ============================================================================
# Enable /health (liveness) and /ready (readiness) endpoints
HEALTH_ENABLED=true

# Include PostgreSQL in health checks
HEALTH_CHECK_DB=true

# Include Redis in health checks
HEALTH_CHECK_REDIS=true

# Check external service dependencies
HEALTH_CHECK_DEPENDENCIES=true

# Health check timeout (seconds)
HEALTH_CHECK_TIMEOUT=5.0

# Startup grace period (seconds)
# Readiness checks will return "not ready" during this period
# Set based on your slowest service startup time
HEALTH_STARTUP_GRACE_PERIOD=30

# ============================================================================
# Centralized Logging Configuration (Loki)
# ============================================================================
# Log format: 'json' (recommended for Loki) or 'text' (human-readable)
# JSON format enables structured logging with automatic field extraction
LOG_FORMAT=json

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# Monitoring & Alerting Configuration
# ============================================================================
# Environment label for metrics/logs (development, staging, production)
ENVIRONMENT=development

# Region label for multi-region deployments
REGION=us-east-1

# ============================================================================
# PagerDuty Integration
# ============================================================================
# CRITICAL: Set these to your actual PagerDuty integration keys in production
# Get these from: PagerDuty → Services → Service → Integrations → Events API V2

# Primary PagerDuty service key (for critical alerts)
PAGERDUTY_SERVICE_KEY=

# Security team PagerDuty service key (for security alerts)
PAGERDUTY_SECURITY_SERVICE_KEY=

# ============================================================================
# Email Alerting Configuration (SMTP)
# ============================================================================
# SMTP server and port
# Gmail: smtp.gmail.com:587
# SendGrid: smtp.sendgrid.net:587
# AWS SES: email-smtp.us-east-1.amazonaws.com:587
SMTP_HOST=smtp.gmail.com:587

# SMTP authentication
SMTP_USERNAME=
SMTP_PASSWORD=

# Alert recipient email addresses
DEFAULT_EMAIL_TO=ops-team@example.com
DATABASE_TEAM_EMAIL=database-team@example.com
COMPLIANCE_TEAM_EMAIL=compliance-team@example.com

# ============================================================================
# Slack Integration (Webhooks)
# ============================================================================
# Create incoming webhooks at: https://api.slack.com/messaging/webhooks
# Each severity level can have its own webhook and channel

# Critical alerts (24/7 on-call)
SLACK_CRITICAL_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_CRITICAL_CHANNEL=#biowerk-critical-alerts

# Warning alerts
SLACK_WARNINGS_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_WARNINGS_CHANNEL=#biowerk-warnings

# Info alerts
SLACK_INFO_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_INFO_CHANNEL=#biowerk-info

# Database alerts
SLACK_DATABASE_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_DATABASE_CHANNEL=#biowerk-database

# Security alerts
SLACK_SECURITY_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_SECURITY_CHANNEL=#biowerk-security

# Compliance alerts (GDPR)
SLACK_COMPLIANCE_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_COMPLIANCE_CHANNEL=#biowerk-compliance

# Business metrics / Product alerts
SLACK_PRODUCT_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_PRODUCT_CHANNEL=#biowerk-product

# ============================================================================
# Grafana Configuration
# ============================================================================
# Admin credentials (CHANGE IN PRODUCTION!)
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=<GENERATE_WITH_openssl_rand_-base64_32>

# Secret key for signing (generate with: openssl rand -base64 32)
GRAFANA_SECRET_KEY=<GENERATE_WITH_openssl_rand_-base64_32>

# Database backend for Grafana (sqlite3 or postgres)
# Use PostgreSQL for production environments
GRAFANA_DB_TYPE=sqlite3
GRAFANA_DB_HOST=postgres:5432
GRAFANA_DB_NAME=grafana
GRAFANA_DB_USER=biowerk
GRAFANA_DB_PASSWORD=

# Log level: debug, info, warn, error
GRAFANA_LOG_LEVEL=info

# ============================================================================
# Monitoring Best Practices
# ============================================================================
# For Production Deployments:
#
# 1. PagerDuty Setup:
#    - Create separate services for Critical, Security, and Database alerts
#    - Configure escalation policies (on-call rotation)
#    - Set up maintenance windows for planned downtime
#    - Test integration before going live
#
# 2. Slack Setup:
#    - Create dedicated channels for each alert severity
#    - Set appropriate channel notifications (mute info, alert on critical)
#    - Add runbook links to alert templates
#    - Configure channel retention policies
#
# 3. Email Setup:
#    - Use dedicated email distribution lists
#    - Configure SPF/DKIM for deliverability
#    - Set up email filtering rules
#    - Monitor for email delivery failures
#
# 4. Alert Tuning:
#    - Start with conservative thresholds
#    - Monitor alert volume and adjust
#    - Implement alert inhibition rules
#    - Review and update alert rules quarterly
#    - Document alert response procedures
#
# 5. Log Retention:
#    - Development: 7 days
#    - Staging: 30 days
#    - Production: 31-90 days (compliance requirements)
#    - Archive old logs to S3/GCS for long-term storage
#
# 6. Metrics Retention:
#    - High-resolution (15s): 7 days
#    - Medium-resolution (5m): 30 days
#    - Low-resolution (1h): 1 year
#    - Use remote storage (Thanos, Cortex) for long-term metrics
#
# 7. Security:
#    - Enable authentication on all monitoring endpoints
#    - Use TLS for all external communication
#    - Rotate API keys and passwords regularly
#    - Implement IP whitelisting where possible
#    - Monitor monitoring system health (meta-monitoring)
#
# 8. High Availability:
#    - Run multiple Prometheus instances with federation
#    - Use Alertmanager clustering (3+ instances)
#    - Replicate Grafana database
#    - Set up Loki in microservices mode for scale
#    - Implement backup and disaster recovery procedures

# ============================================================================
# Backup & Disaster Recovery Configuration
# ============================================================================

# Backup Service Log Level (DEBUG, INFO, WARNING, ERROR)
BACKUP_LOG_LEVEL=INFO

# ============================================================================
# Backup Schedules (Cron Format)
# ============================================================================
# Default schedule: Daily at 2 AM UTC for PostgreSQL, 2:30 AM for MongoDB, 3 AM for Redis
# Format: "minute hour day month weekday"
# Examples:
#   - "0 2 * * *"      Daily at 2 AM
#   - "0 2 * * 0"      Weekly on Sunday at 2 AM
#   - "0 2 1 * *"      Monthly on 1st at 2 AM
#   - "*/30 * * * *"   Every 30 minutes

POSTGRES_BACKUP_ENABLED=true
POSTGRES_BACKUP_SCHEDULE=0 2 * * *

MONGODB_BACKUP_ENABLED=true
MONGODB_BACKUP_SCHEDULE=30 2 * * *

REDIS_BACKUP_ENABLED=true
REDIS_BACKUP_SCHEDULE=0 3 * * *

# ============================================================================
# Backup Retention Policies
# ============================================================================
# Balance storage costs with recovery requirements

# Daily backups retention (in days)
BACKUP_RETENTION_DAYS=30

# Weekly backups retention (in weeks)
BACKUP_RETENTION_WEEKLY=12

# Monthly backups retention (in months)
BACKUP_RETENTION_MONTHLY=12

# ============================================================================
# Backup Options
# ============================================================================

# Compression method: gzip, zstd, none
# zstd recommended for best compression ratio and speed
BACKUP_COMPRESSION=zstd

# Enable AES-256 encryption for backups
# CRITICAL: Set to true in production
BACKUP_ENCRYPTION_ENABLED=true

# Verify backup integrity after creation
BACKUP_VERIFY=true

# Backup verification schedule (weekly recommended)
BACKUP_VERIFICATION_SCHEDULE=0 4 * * 0

# ============================================================================
# Cloud Storage / Off-Site Backups
# ============================================================================

# AWS S3 Configuration
BACKUP_S3_ENABLED=false
BACKUP_S3_BUCKET=biowerk-backups
BACKUP_S3_REGION=us-east-1
# S3 storage class: STANDARD, STANDARD_IA, GLACIER, DEEP_ARCHIVE
BACKUP_S3_STORAGE_CLASS=STANDARD_IA

# AWS Credentials (use IAM roles in production)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# Azure Blob Storage (alternative to S3)
BACKUP_AZURE_ENABLED=false
BACKUP_AZURE_STORAGE_ACCOUNT=
BACKUP_AZURE_STORAGE_KEY=
BACKUP_AZURE_CONTAINER=biowerk-backups

# Google Cloud Storage (alternative to S3/Azure)
BACKUP_GCS_ENABLED=false
BACKUP_GCS_BUCKET=biowerk-backups
BACKUP_GCS_PROJECT=

# ============================================================================
# Disaster Recovery Configuration
# ============================================================================

# Recovery Time Objective (RTO) - target time to restore service
# Format: 1h, 2h, 4h, 8h, 24h
DR_RTO_TARGET=4h

# Recovery Point Objective (RPO) - acceptable data loss window
# Format: 15m, 30m, 1h, 4h, 24h
DR_RPO_TARGET=1h

# Enable automated DR testing
DR_TEST_ENABLED=true

# DR test schedule (weekly recommended - Sunday at 4 AM)
DR_TEST_SCHEDULE=0 4 * * 0

# ============================================================================
# Backup Best Practices for Production
# ============================================================================
#
# 1. Encryption:
#    - ALWAYS enable encryption (BACKUP_ENCRYPTION_ENABLED=true)
#    - Store encryption keys in secure vault (AWS Secrets Manager, HashiCorp Vault)
#    - Never commit encryption keys to version control
#    - Rotate encryption keys every 90 days
#    - Maintain offline backup of encryption keys
#
# 2. Off-Site Storage:
#    - ALWAYS enable cloud storage (S3/Azure/GCS)
#    - Use separate AWS account or subscription for backup storage
#    - Enable versioning on S3 bucket
#    - Configure lifecycle policies for cost optimization
#    - Enable MFA Delete on S3 bucket for ransomware protection
#
# 3. Retention Strategy:
#    - Follow 3-2-1 rule: 3 copies, 2 different media, 1 off-site
#    - Daily: 30 days (meet typical compliance requirements)
#    - Weekly: 12 weeks (3 months of weekly snapshots)
#    - Monthly: 12 months (1 year of monthly snapshots)
#    - Consider legal/compliance requirements (may need 7+ years)
#
# 4. Testing:
#    - Test restores WEEKLY, not just when disaster strikes
#    - Automate restore testing (DR_TEST_ENABLED=true)
#    - Document actual restore times vs RTO targets
#    - Practice full DR drills quarterly
#    - Maintain DR runbook and keep it updated
#
# 5. Monitoring:
#    - Configure PagerDuty/Slack alerts for backup failures
#    - Alert if backup age exceeds RPO target
#    - Monitor backup size trends (unexpected changes)
#    - Track backup/restore duration metrics
#    - Set up dead man's switch for backup orchestrator
#
# 6. Security:
#    - Use IAM roles instead of access keys when possible
#    - Implement least privilege access (backup-only permissions)
#    - Enable S3 bucket logging and CloudTrail
#    - Restrict backup deletion to senior staff only
#    - Audit backup access regularly
#
# 7. Compliance:
#    - GDPR: Include GDPR export data in backups
#    - HIPAA: Ensure encryption at rest and in transit
#    - SOC 2: Document backup procedures and testing
#    - PCI DSS: Secure key management for encrypted backups
#    - Document retention policies per regulation
#
# 8. Cost Optimization:
#    - Use appropriate S3 storage classes:
#      * STANDARD_IA: Daily/weekly backups (30-90 days)
#      * GLACIER: Monthly backups (90 days - 1 year)
#      * DEEP_ARCHIVE: Long-term compliance (1+ years)
#    - Enable compression (zstd recommended)
#    - Clean up old backups automatically
#    - Monitor storage costs and adjust retention
#
# 9. Documentation:
#    - Maintain current DR runbook (docs/DISASTER_RECOVERY.md)
#    - Document RTO/RPO targets and actual performance
#    - Keep emergency contact list updated
#    - Document backup encryption key recovery process
#    - Review and update documentation quarterly
#
# 10. Validation:
#     - Verify checksums on all backups
#     - Test integrity with database-specific tools
#     - Validate backup size is reasonable
#     - Check for backup corruption regularly
#     - Maintain backup audit trail
